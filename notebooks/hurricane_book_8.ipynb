{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LogNorm, PowerNorm\n",
    "\n",
    "\n",
    "\n",
    "from scipy import interpolate\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_historical_diagram():\n",
    "    '''\n",
    "    do all of the stuff for the graphics of making a hurricane plot\n",
    "    \n",
    "    takes in the year dataframe in order to get the necessary data\n",
    "    \n",
    "    returns ax which is the figure axis that the current hurricane track will be added upon\n",
    "    '''\n",
    "    # establish the figure\n",
    "    figure = plt.figure(figsize=(19.2, 12.0), dpi=100)\n",
    "    \n",
    "    axis = figure.add_subplot(111)\n",
    "    axis.set_facecolor(\"#000000\")\n",
    "    \n",
    "    axis.set_xlim(-120.0, -30.0)\n",
    "    axis.set_ylim(0, 50.0)\n",
    "    \n",
    "    return figure, axis\n",
    "    \n",
    "def heatmap(ax, storm):\n",
    "    '''\n",
    "    make a heatmap of storm track?\n",
    "    '''\n",
    "    x, y = storm_heat(storm)\n",
    "    \n",
    "    ax.hexbin(x, y, gridsize=50, bins=\"log\", cmap=\"inferno\")\n",
    "    \n",
    "    # ax.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\n",
    "    \n",
    "def storm_heat(storm):\n",
    "    '''\n",
    "    make storm track into two lists that will be turned into a heatmap\n",
    "    '''\n",
    "    _x = []\n",
    "    _y = []\n",
    "    \n",
    "    if len(storm) >= 5:\n",
    "        interpolation_multiplier = 120\n",
    "\n",
    "        _lat = storm.Latitude.apply(lambda x: float(x))\n",
    "        _long = storm.Longitude.apply(lambda x: float(x))\n",
    "\n",
    "#         _strengths = storm.loc[:,\"Wind(WMO)\"].apply(lambda x: int(((x / 165.0) * 5.0)**2))\n",
    "        \n",
    "        # since the 'saffir_simpson_cat' column ranges from -1 to 5 add 2 to it\n",
    "        # so that the range is 1 <-> 7 which a computer understands better\n",
    "        _strengths = storm.loc[:, \"saffir_simpson_cat\"].apply(lambda x: int((x + 2)**2))\n",
    "        \n",
    "        new_length = interpolation_multiplier * len(_lat)\n",
    "\n",
    "        x = np.arange(len(_lat))\n",
    "\n",
    "        # figure out length of new arrays\n",
    "        new_x = np.linspace(x.min(), x.max(), new_length)\n",
    "\n",
    "        # actually do the interpolation\n",
    "        new_lat = interpolate.interp1d(x, _lat, kind='cubic')(new_x)\n",
    "        new_long = interpolate.interp1d(x, _long, kind='cubic')(new_x)\n",
    "        new_strs = interpolate.interp1d(x, _strengths, kind='cubic')(new_x)\n",
    "        \n",
    "        for idx in range(len(new_lat)):\n",
    "            _strength = int(new_strs[idx])\n",
    "            for idy in range(_strength):\n",
    "                _x.append(new_long[idx])\n",
    "                _y.append(new_lat[idx])\n",
    "\n",
    "    return _x, _y\n",
    "\n",
    "def safsimpsonize(wind):\n",
    "    '''\n",
    "    Takes in wind speed\n",
    "    \n",
    "    Returns saffir-simpson hurricane category with 0 and -1 for tropical storm/depression\n",
    "    which doesn't make perfect sense as scales go but this maintains categories but allows\n",
    "    the model/visualization to detect when something is a tropical depression\n",
    "    \n",
    "    According to: https://en.wikipedia.org/wiki/Saffir%E2%80%93Simpson_scale\n",
    "    \n",
    "    Return:  Category:   Wind Speed Range: (mph, seriously?)\n",
    "    5        Cat 5      157 <= v\n",
    "    4        Cat 4      130 <= v < 157\n",
    "    3        Cat 3      111 <= v < 130\n",
    "    2        Cat 2      96 <= v < 111\n",
    "    1        Cat 1      74 <= v < 96\n",
    "    0        Storm      39 <= v < 74\n",
    "    -1       Depression v < 39\n",
    "    '''\n",
    "    if wind >= 157:\n",
    "        return 5\n",
    "    elif wind >= 130:\n",
    "        return 4\n",
    "    elif wind >= 111:\n",
    "        return 3\n",
    "    elif wind >= 96:\n",
    "        return 2\n",
    "    elif wind >= 74:\n",
    "        return 1\n",
    "    elif wind >= 39:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def load_hurricane_data(_path=\"../data/allstorms.csv\"):\n",
    "    data = pd.read_csv(_path)\n",
    "\n",
    "    # data dictionary\n",
    "    # N/A,Year,#,BB,BB,N/A,YYYY-MM-DD HH:MM:SS,N/A,deg_north,deg_east,kt,mb,N/A,%,%,N/A\n",
    "\n",
    "    data.loc[:,\"Season\"] = data.loc[:,\"Season\"].apply(lambda x: int(x))\n",
    "\n",
    "#     print data.loc[:,\"Basin\"].unique()\n",
    "    # array(['BB', ' SI', ' NA', ' EP', ' SP', ' WP', ' NI', ' SA'], dtype=object)\n",
    "\n",
    "    data.loc[:, \"Basin\"] = data.loc[:, \"Basin\"].apply(lambda x: x.replace(\" \", \"\"))\n",
    "\n",
    "#     print data.loc[:,\"Basin\"].unique()\n",
    "    # ['BB' 'SI' 'NA' 'EP' 'SP' 'WP' 'NI' 'SA']\n",
    "\n",
    "    data_na = data[data.loc[:, \"Basin\"] == \"NA\"]\n",
    "    \n",
    "    data_na.loc[:,\"saffir_simpson_cat\"] = data_na[\"Wind(WMO)\"].apply(lambda x: safsimpsonize(x))\n",
    "\n",
    "    # try to give back memory\n",
    "    del data\n",
    "    \n",
    "    return data_na\n",
    "\n",
    "def get_data_as_yearlist(start_year, end_year):\n",
    "    '''\n",
    "    take in data frame (north atlantic most likely) and turn it into a list of dataframes\n",
    "    with each entry being a dataframe holding a year's data\n",
    "    '''\n",
    "    # load data frame that we want\n",
    "    data_na = load_hurricane_data()\n",
    "    \n",
    "    # make a list for the years\n",
    "    years = []\n",
    "\n",
    "    # step through the Seasons (years) and make a new dataframe for each one\n",
    "    for year in data_na.loc[:, \"Season\"].unique():\n",
    "        temp = data_na[data_na.loc[:, \"Season\"] == year]\n",
    "        years.append(temp)\n",
    "\n",
    "    # get rid of a nan DataFrame\n",
    "    years.pop(0)\n",
    "\n",
    "    #loop through years in future gif\n",
    "    start = 164 - (2016 - start_year)\n",
    "    end = 164 - (2016 - end_year)\n",
    "    \n",
    "    if start != end:\n",
    "        temp = years[start:end]\n",
    "    else:\n",
    "        # handle case where only one year is wanted\n",
    "        temp = []\n",
    "        temp.append(years[start])\n",
    "    \n",
    "    # try to give back memory\n",
    "    del years\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def get_storms_from_year(year_df):\n",
    "    '''\n",
    "    year_df is the dataframe with a year's data\n",
    "    \n",
    "    returns a list of smaller dataframes each consisting of a \n",
    "    unique storm track\n",
    "    '''\n",
    "    storms = []\n",
    "    \n",
    "    # step through the year and make a dataframe for each storm\n",
    "    for storm in year_df.loc[:,\"Serial_Num\"].unique():\n",
    "        storms.append(year_df[year_df.loc[:, \"Serial_Num\"] == storm])\n",
    "        \n",
    "    return storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_map_data(_start=1915, _end=2016):\n",
    "    # extract data from dataframe into a usable matrix form\n",
    "    #\n",
    "    # establish the range of years\n",
    "    # ^ moved to function parameter\n",
    "\n",
    "    # make a list of year dataframes for the range\n",
    "    years = get_data_as_yearlist(_start, _end)\n",
    "\n",
    "    big_x = []\n",
    "    big_y = []\n",
    "\n",
    "    # make a temp list to hold the storm dataframes from a single year\n",
    "    for _idx, year in enumerate(years):\n",
    "\n",
    "        storms = get_storms_from_year(year)\n",
    "\n",
    "        for z, storm in enumerate(storms):\n",
    "\n",
    "            x, y = storm_heat(storm)\n",
    "\n",
    "            big_x += x\n",
    "            big_y += y\n",
    "\n",
    "    x = np.array(big_x)\n",
    "    y = np.array(big_y)\n",
    "\n",
    "    temp_x = []\n",
    "    temp_y = []\n",
    "\n",
    "    for idx in range(x.shape[0]):\n",
    "        if 10 <= y[idx] <= 50 and -120 <= x[idx] <= -30:\n",
    "            temp_x.append(x[idx])\n",
    "            temp_y.append(y[idx])\n",
    "\n",
    "    x = np.array(temp_x)\n",
    "    y = np.array(temp_y)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def gridsearch_draw_map(grid_scale, x, y, _xs, _ys, gam1=1.0, gam2=2.0, file_name=None, color_map=0):\n",
    "    '''\n",
    "    Perform a randomized grid search of hyper parameters used to make diagrams\n",
    "    \n",
    "    Looking for hyperparameters that would not have occurred through normal thought\n",
    "    processes.\n",
    "    \n",
    "    Output:\n",
    "    writes a png with the resulting image as well as the hyperparameters to make it\n",
    "    '''\n",
    "\n",
    "    color_dict = {0:\"inferno\", 1:\"plasma\", 2:\"magma\", 3:\"viridis\", 4:\"hot\", 5:\"afmhot\",\n",
    "                 6:\"gist_heat\", 7:\"copper\", 8:\"bone\", 9:\"gnuplot\", 10:\"gnuplot2\",\n",
    "                  11:\"CMRmap\", 12:\"pink\", 13:\"spring\", 14:\"autumn_r\", 15:\"cool\",\n",
    "                 16:\"Wistia\", 17:\"seismic\", 18:\"RdGy_r\", 19:\"BrBG_r\", 20:\"RdYlGn_r\",\n",
    "                 21:\"PuOr\", 22:\"brg\", 23:\"hsv\", 24:\"cubehelix\", 25:\"gist_earth\",\n",
    "                 26:\"ocean\", 27:\"gist_stern\", 28:\"gist_rainbow_r\", 29:\"jet\", \n",
    "                 30:\"nipy_spectral\", 31:\"gist_ncar\"}\n",
    "    \n",
    "    _cmap = color_dict[color_map]\n",
    "    \n",
    "    \n",
    "    grid = np.zeros((int(40 * grid_scale), int(90 * grid_scale)))\n",
    "    grid2 = np.zeros((int(40 * grid_scale), int(90 * grid_scale)))\n",
    "    \n",
    "#     print grid.shape\n",
    "\n",
    "    for idx in range(x.shape[0]):\n",
    "        _Y = int(50 * grid_scale) - 2 - int(y[idx] * grid_scale)\n",
    "        # was a -1 adjustment because counting starts at 0, changing to -2 because\n",
    "        # storms tend to start on the right side and not hit the left side\n",
    "        # so this is a hack-ey fix for what seems like a rounding problem\n",
    "        _X = int(120 * grid_scale) - 2 + int(x[idx] * grid_scale)\n",
    "\n",
    "        grid[_Y, _X] += 1\n",
    "        \n",
    "    for idy in range(_xs.shape[0]):\n",
    "        _Ys = int(50 * grid_scale) - 2 - int(_ys[idy] * grid_scale)\n",
    "        # was a -1 adjustment because counting starts at 0, changing to -2 because\n",
    "        # storms tend to start on the right side and not hit the left side\n",
    "        # so this is a hack-ey fix for what seems like a rounding problem\n",
    "        _Xs = int(120 * grid_scale) - 2 + int(_xs[idy] * grid_scale)\n",
    "        \n",
    "        grid2[_Ys, _Xs] += 1\n",
    "\n",
    "    # establish the figure\n",
    "    fig = plt.figure(figsize=(19.2, 24.0), dpi=100)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_facecolor(\"#000000\")\n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "\n",
    "    ax.set_xlim(-120.0, -30.0)\n",
    "    ax.set_ylim(10, 50.0)\n",
    "\n",
    "#     map_image = imread(\"../data/grey_blue_na_2.png\")\n",
    "\n",
    "#     ax.imshow(map_image, extent=[-120, -30, 10, 50], aspect=\"auto\")\n",
    "\n",
    "    ax.imshow(grid, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-120, -30, 10, 30], alpha=1.0, aspect=\"auto\")\n",
    "    ax.imshow(grid2, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-120, -30, 30, 50], alpha=1.0, aspect=\"auto\")\n",
    "    \n",
    "    # write out file info so we can see how a map was made later w/ grid search\n",
    "    desc = \"grid_scale: {:2.2f}\\n gam1: {:2.2f}\\n gam2: {:2.2f}\\n _cmap: {}\".format(grid_scale, gam1, gam2, _cmap)\n",
    "    ax.annotate(desc, xy=(-119, 43.8), size=30, color='#D0D0D0')\n",
    "    \n",
    "    if file_name != None:\n",
    "        fig.savefig(\"../imgs/test/grid_heat/{}\".format(file_name), pad_inches=0, transparent=True)\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    del _X, _Y, _Xs, _Ys\n",
    "    \n",
    "def run_map_grid_search(x, y, xs, xy, _start=0, _end=1000):\n",
    "    '''\n",
    "    Pick hyper parameters to use in a randomized grid search to try to \n",
    "    find a good looking heatmap setup\n",
    "    '''\n",
    "    \n",
    "    for rdx in range(_start, _end):\n",
    "        print \"Making map [{:0>4}]/1000\\r\".format(rdx),\n",
    "        _filename = \"cmap_gsearch_num{:0>4d}.png\".format(rdx)\n",
    "        \n",
    "        # pick grid_scale from lognormal distribution\n",
    "        # with these values min/max of 10,000 test sample is ~0.5, 21.4\n",
    "        mu, sigma = 1.2, 0.5\n",
    "        # returns a numpy array so steal the first value in that to actually get the value\n",
    "        _grid_scale = np.random.lognormal(mu, sigma, 1)[0]\n",
    "\n",
    "        # gamma 1: pull from a normal distribution centered around 1.0\n",
    "        norm_mu, norm_sigma = 1.5, 0.5\n",
    "\n",
    "        _gam1 = np.random.normal(norm_mu, norm_sigma, 1)[0]\n",
    "\n",
    "        # we don't want the value to go negative so if it is negative set it to 1.0\n",
    "        if _gam1 <= 0:\n",
    "            _gam1 = 1.0\n",
    "\n",
    "        #, gamma 2, cmap? if so what cmap?\n",
    "        g2_mu, g2_sigma = 3.0, 1.0\n",
    "\n",
    "        do_over = 0\n",
    "\n",
    "        _gam2 = -10\n",
    "\n",
    "        # we want to keep picking until _gam2 is bigger than _gam1\n",
    "        # but prevent infinite loops because that's really annoying\n",
    "        while _gam2 < _gam1 or do_over < 1000:\n",
    "            # break infinite loop\n",
    "            do_over += 1\n",
    "            _gam2 = np.random.normal(g2_mu, g2_sigma, 1)[0]\n",
    "\n",
    "        # about 2/3rds the time use the non-standard color map\n",
    "        _color_map = np.random.randint(0, 32)\n",
    "\n",
    "    #     draw_map(grid_scale, x, y, _xs, _ys, gam1=1.0, gam2=2.0, file_name=None, color_map=0)\n",
    "        draw_map(_grid_scale, x, y, xs, xy, _gam1, _gam2, color_map=_color_map, file_name=_filename)\n",
    "        \n",
    "def easy_buffer(_filename, h, w):\n",
    "    '''\n",
    "    load file with filename, turn into np buffer\n",
    "    \n",
    "    returns:\n",
    "    new np buffer\n",
    "    '''\n",
    "    # load track file\n",
    "    storm_tracks = imread(\"../imgs/xplots4/{}\".format(_filename))\n",
    "    \n",
    "    new_buffer = np.frombuffer(storm_tracks, np.uint8)\n",
    "    \n",
    "    temp_buffer = new_buffer.astype(np.int16).reshape(h, w, 4) #int16 so we dont overflow\n",
    "    \n",
    "#     print \"new_buffer shape:\", temp_buffer.shape\n",
    "    \n",
    "#     print \"max heat:\", temp_buffer[..., 1].max()\n",
    "    \n",
    "    temp_buffer[temp_buffer[:, :, -1] == 0] = 0\n",
    "    \n",
    "    return temp_buffer\n",
    "        \n",
    "def add_layer_to_buffer(_filename):\n",
    "    '''\n",
    "    Takes in a numpy array buffer\n",
    "    loads historical plot from _filename\n",
    "    draws that plot using fig/ax\n",
    "    adds that new buffer to the existing buffer\n",
    "    \n",
    "    Returns:\n",
    "    _buffer with new image added\n",
    "    '''\n",
    "    # load track file\n",
    "    storm_tracks = imread(\"../imgs/xplots4/{}\".format(_filename))\n",
    "    \n",
    "    print \"storm_tracks shape:\", storm_tracks.shape\n",
    "    \n",
    "    # establish the figure\n",
    "    figure = plt.figure(figsize=(19.2, 12.0), dpi=100)\n",
    "\n",
    "    axis = figure.add_subplot(111)\n",
    "    axis.set_facecolor(\"#000000\")\n",
    "    axis.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    figure.subplots_adjust(0, 0, 1, 1)\n",
    "    \n",
    "    axis.set_xlim(-110.0, -30.0)\n",
    "    axis.set_ylim(10, 50.0)\n",
    "#     ax.tick_params(directio\n",
    "    \n",
    "    # get canvas dimensions\n",
    "    w, h = figure.canvas.get_width_height()\n",
    "    \n",
    "    new_buffer = np.frombuffer(figure.canvas.buffer_rgba(), np.uint8)\n",
    "    \n",
    "    # the first buffer which is used for the new track in color\n",
    "    temp_buffer = new_buffer.astype(np.int16).reshape(h, w, -1) #int16 so we dont overflow\n",
    "    temp_buffer[temp_buffer[:, :, -1] == 0] = 0 # Set transparent pixels to 0\n",
    "    \n",
    "\n",
    "\n",
    "    # clean up canvas for new image\n",
    "    axis.clear()\n",
    "    \n",
    "    # test to see if this part is really necessary\n",
    "    axis.set_xlim(-110.0, -30.0)\n",
    "    axis.set_ylim(10.0, 50.0)\n",
    "    \n",
    "    # also probably cargo cult but set the face to black:\n",
    "    axis.patch.set_facecolor('#000000')\n",
    "    \n",
    "    # draw the new tracks on the canvas\n",
    "    axis.imshow(storm_tracks, extent=[-110, -30, 10, 50], aspect=\"auto\")\n",
    "    \n",
    "    # apply the changes to the canvas\n",
    "    figure.canvas.draw()\n",
    "    \n",
    "    # pull the color values from the canvas\n",
    "    img = np.frombuffer(figure.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "    \n",
    "    # maybe yet more cargo cult esp since we're loading tracks w/ transparent backgrounds\n",
    "    img[img[:, :, -1] == 0] = 0 # Set transparent pixels to 0\n",
    "    \n",
    "    temp_buffer += img\n",
    "    \n",
    "    return temp_buffer\n",
    "    \n",
    "def establish_canvas():\n",
    "    '''\n",
    "    basic canvas configuration for matplotlib\n",
    "    \n",
    "    Returns:\n",
    "    fig, ax to use for the diagram\n",
    "    '''\n",
    "    # establish the figure\n",
    "    fig = plt.figure(figsize=(19.2, 12.0), dpi=100)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_facecolor(\"#000000\")\n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    \n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10, 50.0)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    _buffer = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8)\n",
    "    \n",
    "    # the first buffer which is used for the new track in color\n",
    "    main_buffer = _buffer.astype(np.int16).reshape(h, w, -1) #int16 so we dont overflow\n",
    "    main_buffer[main_buffer[:, :, -1] == 0] = 0 # Set transparent pixels to 0\n",
    "    \n",
    "    return fig, ax, main_buffer\n",
    "    \n",
    "def draw_map(grid_scale, gam1=1.0, gam2=2.0, file_name=None, color_map=0, year='', _alpha=1.0):\n",
    "\n",
    "    color_dict = {0:\"inferno\", 1:\"plasma\", 2:\"magma\", 3:\"viridis\", 4:\"hot\", 5:\"afmhot\",\n",
    "                 6:\"gist_heat\", 7:\"copper\", 8:\"bone\", 9:\"gnuplot\", 10:\"gnuplot2\",\n",
    "                  11:\"CMRmap\", 12:\"pink\", 13:\"spring\", 14:\"autumn_r\", 15:\"cool\",\n",
    "                 16:\"Wistia\", 17:\"seismic\", 18:\"RdGy_r\", 19:\"BrBG_r\", 20:\"RdYlGn_r\",\n",
    "                 21:\"PuOr\", 22:\"brg\", 23:\"hsv\", 24:\"cubehelix\", 25:\"gist_earth\",\n",
    "                 26:\"ocean\", 27:\"gist_stern\", 28:\"gist_rainbow_r\", 29:\"jet\", \n",
    "                 30:\"nipy_spectral\", 31:\"gist_ncar\"}\n",
    "    \n",
    "    _cmap = color_dict[color_map]\n",
    "\n",
    "    # make the canvas\n",
    "    fig, ax, first = establish_canvas()    \n",
    "    w, h = fig.canvas.get_width_height()\n",
    "\n",
    "    # load in the background map image file\n",
    "    map_image = imread(\"../data/new_ultra_map.png\")\n",
    "\n",
    "    # draw the map\n",
    "    ax.imshow(map_image, extent=[-110, -30, 10, 50], aspect=\"auto\")\n",
    "    \n",
    "    # paint the canvas so we can pull it into a map buffer\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # get the map image into a buffer\n",
    "    map_img = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "    map_img[map_img[:, :, -1] == 0] = 0  \n",
    "    \n",
    "    print \"max map heat:\", map_img[..., 1].max()\n",
    "    \n",
    "    first += map_img\n",
    "\n",
    "    ###############################################\n",
    "    ### Finish the standard base of the diagram ###\n",
    "    ###############################################\n",
    "    \n",
    "    # make a new buffer for the heatmap itself\n",
    "    ax.clear()\n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10.0, 50.0)\n",
    "    ax.set_facecolor('#000000')\n",
    "#     _heatmap = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8)\n",
    "#     _heatmap = _heatmap.astype(np.int16).reshape(h, w, -1) #int16 so we dont overflow\n",
    "    \n",
    "#     # see if we really need this...\n",
    "#     _heatmap[_heatmap[:, :, -1] == 0] = 0 # Set transparent pixels to 0\n",
    "    _heatmap = np.zeros(shape=(h, w, 4))\n",
    "    \n",
    "    # how many years to include in the heatmap part\n",
    "    for x in range(1):\n",
    "        # which year to start pulling heatmaps\n",
    "        _year_zero = 1915\n",
    "        \n",
    "        # figure out the filename for the current year\n",
    "        _filename = \"xplot_{}.png\".format(_year_zero + x)\n",
    "        \n",
    "        # load the transparent background map and add it's values to the buffer\n",
    "        _heatmap += easy_buffer(_filename, h, w)\n",
    "\n",
    "    print \"max heat coming out of layer processing:\", _heatmap[..., 1].max()\n",
    "    \n",
    "    # currently the buffer is RGBA but since it's grey we only need one channel\n",
    "    _buff = _heatmap[...,1]\n",
    "    # max heat is 1500 so it seems like data is being loaded ok\n",
    "        \n",
    "    # prepare to draw the new buffer\n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10, 50.0)\n",
    "    ax.patch.set_facecolor('#000000')\n",
    "    \n",
    "    # draw the new combined heatmap\n",
    "    ax.imshow(_buff, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-110, -30, 10, 50], alpha=1.0, aspect=\"auto\")\n",
    "    \n",
    "#     fig.savefig(\"../imgs/test/heat_render/test_more.png\", pad_inches=0, transparent=True)\n",
    "    # paint the canvas\n",
    "    fig.canvas.draw()    \n",
    "    \n",
    "    # pull the paint back off the canvas into the buffer\n",
    "    heat_img = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "    \n",
    "    print \"max heat from heatmap buffer:\", heat_img[..., 1].max()\n",
    "    # make transparent parts black\n",
    "#     heat_img[heat_img[:, :, -1] == 0] = 0  \n",
    "    \n",
    "    # and black parts transparent (if they're not already)\n",
    "    heat_img[((heat_img[:,:,0] == 0) & (heat_img[:,:,1] == 0) & (heat_img[:,:,2] == 0))] = 0\n",
    "    \n",
    "    # add the heatmap to the final buffer\n",
    "    first += heat_img\n",
    "    \n",
    "    print \"max final heat:\", first[..., 1].max()\n",
    "    \n",
    "    ###############################################\n",
    "    ### Finish the dynamic heatmap part of plot ###\n",
    "    ###############################################\n",
    "    \n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10.0, 50.0)\n",
    "    ax.patch.set_facecolor('#000000')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ax.imshow(layer_1, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-110, -30, 10, 50], alpha=_alpha, aspect=\"auto\")\n",
    "    \n",
    "    \n",
    "#     for a in range(heat_img.shape[0]):\n",
    "#         for b in range(heat_img.shape[1]):\n",
    "#             # if heatmap is black then make it transparent\n",
    "#             if heat_img[a][b][0] <= 1 and heat_img[a][b][1] <= 1 and heat_img[a][b][2] <= 1:\n",
    "#                 heat_img[a, b, :] = 0\n",
    "                \n",
    "#             # if map_img is relatively high then make heatmap alpha 0\n",
    "#             if map_img[a][b][0] >= 30 and map_img[a][b][1] >= 30 and map_img[a][b][2] >= 30:\n",
    "#                 heat_img[a][b][3] = 128\n",
    "#                 heat_img[a][b][0] = heat_img[a][b][0] // 2\n",
    "#                 heat_img[a][b][1] = heat_img[a][b][1] // 2\n",
    "#                 heat_img[a][b][2] = heat_img[a][b][2] // 2\n",
    "    \n",
    "    first = np.clip(first, 0, 255) # clip buffer back into int8 range\n",
    "                    # wonder if some kind of exp transform\n",
    "                    # might enable hdr-like effect    \n",
    "    \n",
    "    \n",
    "    # make the final draw\n",
    "    ax.imshow(first.astype(np.uint8), extent=[-110, -30, 10, 50], aspect='auto', alpha=1.0)\n",
    "    # write out file info so we can see how a map was made later w/ grid search\n",
    "    desc = \"Year: {}\".format(year)\n",
    "    ax.annotate(desc, xy=(-109, 48), size=40, color='#707070')\n",
    "    \n",
    "    if file_name != None:\n",
    "        fig.savefig(\"../imgs/test/heat_render/{}\".format(file_name), pad_inches=0, transparent=True)\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max map heat: 255\n",
      "max heat coming out of layer processing: 146.0\n",
      "max heat from heatmap buffer: 254\n",
      "max final heat: 354\n"
     ]
    }
   ],
   "source": [
    "draw_map(10.28, 3.5, 4.0, color_map=0, year=\"2002-2006\", _alpha=1.0, file_name=\"test_0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1530 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f523dcb2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method FigureCanvasAgg.tostring_argb of <matplotlib.backends.backend_agg.FigureCanvasAgg object at 0x7f523dcb2290>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig.canvas.tostring_argb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
