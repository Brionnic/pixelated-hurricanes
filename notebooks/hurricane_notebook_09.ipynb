{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype and implement the new data structure for 'shockwave.py' to hopefully speed things up 5x+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LogNorm, PowerNorm\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "from scipy import interpolate\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.misc import imread\n",
    "\n",
    "import time\n",
    "\n",
    "from FFMWrapper import FFMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_map_data(_start=1915, _end=2016):\n",
    "    # establish the range of years\n",
    "    # ^ moved to function parameter\n",
    "\n",
    "    # make a list of year dataframes for the range\n",
    "    years = get_data_as_yearlist(_start, _end)\n",
    "\n",
    "#     print grid.shape\n",
    "\n",
    "    tome_of_storms = []\n",
    "\n",
    "    # make a temp list to hold the storm dataframes from a single year\n",
    "    for _idx, year in enumerate(years):\n",
    "\n",
    "        storms = get_storms_from_year(year)\n",
    "\n",
    "        tome_of_storms += storms\n",
    "\n",
    "    return tome_of_storms\n",
    "\n",
    "def get_data_as_yearlist(start_year, end_year):\n",
    "    '''\n",
    "    take in data frame (north atlantic most likely) and turn it into a list of dataframes\n",
    "    with each entry being a dataframe holding a year's data\n",
    "    '''\n",
    "    # load data frame that we want\n",
    "    data_na = load_hurricane_data()\n",
    "\n",
    "    # make a list for the years\n",
    "    years = []\n",
    "\n",
    "    print data_na.loc[:, \"Season\"].unique()\n",
    "    \n",
    "    # step through the Seasons (years) and make a new dataframe for each one\n",
    "    for year in data_na.loc[:, \"Season\"].unique():\n",
    "        temp = data_na[data_na.loc[:, \"Season\"] == year]\n",
    "        years.append(temp)\n",
    "\n",
    "    # get rid of a nan DataFrame\n",
    "    years.pop(0)\n",
    "\n",
    "    #loop through years in future gif\n",
    "    start = 164 - (2016 - start_year)\n",
    "    end = 164 - (2017 - end_year)\n",
    "\n",
    "    if start != end:\n",
    "        temp = years[start:end]\n",
    "    else:\n",
    "        # handle case where only one year is wanted\n",
    "        temp = []\n",
    "        temp.append(years[start])\n",
    "\n",
    "    # try to give back memory\n",
    "    del years\n",
    "\n",
    "    return temp\n",
    "\n",
    "def load_hurricane_data(_path=\"../data/allstorms.csv\"):\n",
    "    data = pd.read_csv(_path)\n",
    "\n",
    "    # data dictionary\n",
    "    # N/A,Year,#,BB,BB,N/A,YYYY-MM-DD HH:MM:SS,N/A,deg_north,deg_east,kt,mb,N/A,%,%,N/A\n",
    "\n",
    "#     print data.loc[:,\"Basin\"].unique()\n",
    "    # array(['BB', ' SI', ' NA', ' EP', ' SP', ' WP', ' NI', ' SA'], dtype=object)\n",
    "\n",
    "    data.loc[:, \"Basin\"] = data.loc[:, \"Basin\"].apply(lambda x: x.replace(\" \", \"\"))\n",
    "\n",
    "#     print data.loc[:,\"Basin\"].unique()\n",
    "    # ['BB' 'SI' 'NA' 'EP' 'SP' 'WP' 'NI' 'SA']\n",
    "\n",
    "    # since we're only looking at North Atlantic in this case\n",
    "    data_na = data[data.loc[:, \"Basin\"] == \"NA\"]\n",
    "    \n",
    "    data_na.loc[:,\"Season\"] = data_na.loc[:,\"Season\"].apply(lambda x: int(x))\n",
    "\n",
    "    # convert wind speed stuff into storm category information\n",
    "    data_na.loc[:,\"saffir_simpson_cat\"] = data_na[\"Wind(WMO)\"].apply(lambda x: safsimpsonize(x))\n",
    "\n",
    "    # try to give back memory\n",
    "    del data\n",
    "\n",
    "    return data_na\n",
    "\n",
    "def get_storms_from_year(year_df):\n",
    "    '''\n",
    "    year_df is the dataframe with a year's data\n",
    "\n",
    "    returns a list of smaller dataframes each consisting of a\n",
    "    unique storm track\n",
    "    '''\n",
    "    storms = []\n",
    "\n",
    "    # step through the year and make a dataframe for each storm\n",
    "    for storm in year_df.loc[:,\"Serial_Num\"].unique():\n",
    "        \n",
    "        # make a temp storm since it's needed more than once\n",
    "        temp_storm = year_df[year_df.loc[:, \"Serial_Num\"] == storm]\n",
    "        if temp_storm.count()[0] >= 5:\n",
    "            storms.append(temp_storm)\n",
    "            \n",
    "        del temp_storm\n",
    "\n",
    "    return storms\n",
    "\n",
    "def safsimpsonize(wind):\n",
    "    '''\n",
    "    Takes in wind speed\n",
    "\n",
    "    Returns saffir-simpson hurricane category with 0 and -1 for tropical storm/depression\n",
    "    which doesn't make perfect sense as scales go but this maintains categories but allows\n",
    "    the model/visualization to detect when something is a tropical depression\n",
    "\n",
    "    According to: https://en.wikipedia.org/wiki/Saffir%E2%80%93Simpson_scale\n",
    "\n",
    "    Return:  Category:   Wind Speed Range: (mph, seriously?)\n",
    "    5        Cat 5      157 <= v\n",
    "    4        Cat 4      130 <= v < 157\n",
    "    3        Cat 3      111 <= v < 130\n",
    "    2        Cat 2      96 <= v < 111\n",
    "    1        Cat 1      74 <= v < 96\n",
    "    0        Storm      39 <= v < 74\n",
    "    -1       Depression v < 39\n",
    "    '''\n",
    "    if wind >= 157:\n",
    "        return 5\n",
    "    elif wind >= 130:\n",
    "        return 4\n",
    "    elif wind >= 111:\n",
    "        return 3\n",
    "    elif wind >= 96:\n",
    "        return 2\n",
    "    elif wind >= 74:\n",
    "        return 1\n",
    "    elif wind >= 39:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_storm_data(list_of_storms):\n",
    "    '''\n",
    "    In the old way storm paths were interpolated for every frame.  This is really wasteful.  \n",
    "    Instead make a list of 2d numpy arrays.  Each numpy array is shape (nobs, 2) where the\n",
    "    columns are lat, and long\n",
    "    \n",
    "    Returns a list of 2d numpy arrays\n",
    "    '''\n",
    "    # data is in 6 hour intervals, interpolate to 10 minute time slices\n",
    "    interpolation_multiplier = 4 * 6 # 10min per time slice\n",
    "    \n",
    "    # where the processed tracks will go\n",
    "    storm_tracks = []\n",
    "    \n",
    "    for storm in list_of_storms:\n",
    "        # make sure that all of the numbers are in a float format\n",
    "        # otherwise weird stuff can happen later (comparing float to str for example)\n",
    "        # since long is a variable name use _ to prevent use of reserved type\n",
    "        _lat = np.array(storm.Latitude.apply(lambda x: float(x)))\n",
    "        _long = np.array(storm.Longitude.apply(lambda x: float(x)))\n",
    "\n",
    "        # how long is the new list going to be?\n",
    "        new_length = interpolation_multiplier * len(_lat)\n",
    "\n",
    "        # to help guide the interpolate method\n",
    "        x = np.arange(len(_lat))\n",
    "\n",
    "        # figure out length of new arrays\n",
    "        new_x = np.linspace(x.min(), x.max(), new_length)\n",
    "\n",
    "        # actually do the interpolation\n",
    "        # these np arrays should all be the same length\n",
    "        new_lat = interpolate.interp1d(x, _lat, kind='cubic')(new_x)\n",
    "        new_long = interpolate.interp1d(x, _long, kind='cubic')(new_x)\n",
    "\n",
    "        #############\n",
    "        ### NOTE: ###\n",
    "        #############\n",
    "\n",
    "        # considered removing duplicate entries (probably a significant number)\n",
    "        # but rejected because the visual velocity of the storms would be lost\n",
    "        # in other words a storm that, for a period of time, was moving slowly\n",
    "        # would probably have a lot of redundant lat/long entries.  But later\n",
    "        # it might start moving more quickly.  This would be indicated by the\n",
    "        # visual length of the trail. A short tail is a relatively slow movement\n",
    "        # a long trail is relatively fast movement.  Removing duplicates would\n",
    "        # make velocities appear constant, which would be undesirable (most likely)\n",
    "#         if np.sum(np.isclose([5.0, 6.02], boink[-1,:], atol=0.01)) == 2:\n",
    "#             print \"True\"\n",
    "        temp = np.column_stack([new_lat, new_long])\n",
    "\n",
    "        storm_tracks.append(temp)\n",
    "\n",
    "        del temp\n",
    "    \n",
    "    return storm_tracks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_rail_grid(storm, storm_num, grid_scale=7.0, _baseline=2):\n",
    "    '''\n",
    "    Make initial 'rail system' for storms to ride\n",
    "    '''\n",
    "    # 40 is the total latitude lines, 80 is the total longitude lines\n",
    "    temp_grid = np.zeros((int(40 * grid_scale), int(80 * grid_scale)), dtype=np.int16)\n",
    "\n",
    "    if len(storm) >= 5:\n",
    "        # interpolate the storm tracks like crazy\n",
    "        interpolation_multiplier = 24 * 6 # 10 min per tick now\n",
    "\n",
    "        # get the data out of the data frame in a usable form\n",
    "        _lat = np.array(storm.Latitude.apply(lambda x: float(x)))\n",
    "        _long = np.array(storm.Longitude.apply(lambda x: float(x)))\n",
    "\n",
    "        # how long is the new list going to be?\n",
    "        new_length = interpolation_multiplier * len(_lat)\n",
    "\n",
    "        # to help guide the interpolate method\n",
    "        x = np.arange(len(_lat))\n",
    "\n",
    "        # figure out length of new arrays\n",
    "        new_x = np.linspace(x.min(), x.max(), new_length)\n",
    "\n",
    "        # actually do the interpolation\n",
    "        # these np arrays should all be the same length\n",
    "        new_lat = interpolate.interp1d(x, _lat, kind='cubic')(new_x)\n",
    "        new_long = interpolate.interp1d(x, _long, kind='cubic')(new_x)\n",
    "\n",
    "        # new_strs = np.zeros(shape=(new_length, 1), dtype=np.int16)\n",
    "        #\n",
    "        # # add the baseline value to the trail to make a sort of 'rail' for\n",
    "        # # the light to ride on. This will help enable the looping?\n",
    "        # new_strs += 2\n",
    "\n",
    "        # so now each storm has 3 things, a (longer) list of: lats, longs\n",
    "        # we can make this a dictionary and add it to a list so we don't\n",
    "        # have to keep redoing this over and over\n",
    "        #\n",
    "        # we don't have to include strengths because this is the default\n",
    "        # and strength is always going to be the baseline\n",
    "        storm_dict = {\"id\":storm_num, \"lat\":new_lat, \"long\":new_long}\n",
    "\n",
    "        for idx in range(new_length):\n",
    "            if 10 <= new_lat[idx] <= 50 and -110 <= new_long[idx] <= -30:\n",
    "                # figure out the X/Y coordinates for the current lat/long location\n",
    "                _Y = int(50 * grid_scale) - 2 - int(new_lat[idx] * grid_scale)\n",
    "                # was a -1 adjustment because counting starts at 0, changing to -2 because\n",
    "                # storms tend to start on the right side and not hit the left side\n",
    "                # so this is a hack-ey fix for what seems like a rounding problem\n",
    "                _X = int(110 * grid_scale) - 2 + int(new_long[idx] * grid_scale)\n",
    "\n",
    "                # since we're initializing we know that the strength will\n",
    "                # always be the baseline\n",
    "                temp_grid[_Y, _X] = _baseline\n",
    "\n",
    "    # else:\n",
    "    #     # it's a super short storm, don't waste time/resources\n",
    "    #     pass\n",
    "\n",
    "    return temp_grid, storm_dict\n",
    "\n",
    "def process_storms(_tome_of_storms, current_frame, num_frames):\n",
    "    '''\n",
    "    takes in a list of ALL storms in the time range and then process the data\n",
    "    so that we have the current proportion of frames\n",
    "\n",
    "    Return this data as a grid\n",
    "    '''\n",
    "    # sort of a constant but might need to be adjusted\n",
    "    # best range seems to be 3.0 - 7.0 but up to 10 is still good\n",
    "    # grid_scale = 0.50\n",
    "    grid_scale = 13.0\n",
    "\n",
    "    # 40 is the total latitude lines, 80 is the total longitude lines\n",
    "    grid = np.zeros((int(40 * grid_scale), int(80 * grid_scale)), dtype=np.float64)\n",
    "\n",
    "    # make a dictionary of all storm tracks\n",
    "\n",
    "    for idx, storm in enumerate(_tome_of_storms):\n",
    "\n",
    "        # total amount of storm frames to use...\n",
    "        ratio = current_frame / (num_frames * 1.0)\n",
    "\n",
    "        temp_grid, storm_dict = make_rail_grid(storm, idx, grid_scale)\n",
    "\n",
    "        grid += storm_heat(storm, grid_scale, ratio)\n",
    "\n",
    "    print \"max of grid:\", grid[...].max()\n",
    "\n",
    "    grid = np.clip(grid, 0, 255)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def storm_heat(storm, grid_scale=7.0, ratio=1.0):\n",
    "    '''\n",
    "    make storm track into two lists that will be turned into a heatmap\n",
    "    '''\n",
    "    # 40 is the total latitude lines, 80 is the total longitude lines\n",
    "    temp_grid = np.zeros((int(40 * grid_scale), int(80 * grid_scale)), dtype=np.int16)\n",
    "\n",
    "    if len(storm) >= 5:\n",
    "        # interpolate the storm tracks like crazy\n",
    "        interpolation_multiplier = 24 * 6 # 10 min per tick now\n",
    "\n",
    "        # get the data out of the data frame in a usable form\n",
    "        _lat = np.array(storm.Latitude.apply(lambda x: float(x)))\n",
    "        _long = np.array(storm.Longitude.apply(lambda x: float(x)))\n",
    "        # since the 'saffir_simpson_cat' column ranges from -1 to 5 add 2 to it\n",
    "        # so that the range is 1 <-> 7 which a computer understands better\n",
    "        # _strengths = np.array(storm.loc[:, \"saffir_simpson_cat\"].apply(lambda x: int((x + 2)**2)))\n",
    "\n",
    "        # clean_lat = []\n",
    "        # clean_long = []\n",
    "        # # clean_storms = []\n",
    "        # # before we expand by a large magnitude the size of the lists lets dump\n",
    "        # # the values that aren't in the picture anyways\n",
    "        # for idx in range(len(_lat)):\n",
    "        #     if 10 <= _lat[idx] <= 50 and -110 <= _long[idx] <= -30:\n",
    "        #         clean_lat.append(_lat[idx])\n",
    "        #         clean_long.append(_long[idx])\n",
    "        #         # clean_storms.append(_strengths[idx])\n",
    "        #\n",
    "        # # dump the memory for the obsolete lists\n",
    "        # del _lat, _long, # _strengths\n",
    "        #\n",
    "        # _lat = np.array(clean_lat)\n",
    "        # _long = np.array(clean_long)\n",
    "        # # _strengths = np.array(clean_storms)\n",
    "        #\n",
    "        # del clean_lat, clean_long, # clean_storms\n",
    "\n",
    "#         _strengths = storm.loc[:,\"Wind(WMO)\"].apply(lambda x: int(((x / 165.0) * 5.0)**2))\n",
    "\n",
    "        # how long is the new list going to be?\n",
    "        new_length = interpolation_multiplier * len(_lat)\n",
    "\n",
    "        # some storm tracks might never appear on our screen so length of\n",
    "        # _lat or _long might be zero, if so then break for this hurricane\n",
    "        if len(_lat) > 5 and len(_long) > 5:\n",
    "\n",
    "\n",
    "            # to help guide the interpolate method\n",
    "            x = np.arange(len(_lat))\n",
    "\n",
    "            # figure out length of new arrays\n",
    "            new_x = np.linspace(x.min(), x.max(), new_length)\n",
    "\n",
    "            # actually do the interpolation\n",
    "            # these np arrays should all be the same length\n",
    "            new_lat = interpolate.interp1d(x, _lat, kind='cubic')(new_x)\n",
    "            new_long = interpolate.interp1d(x, _long, kind='cubic')(new_x)\n",
    "            # new_strs = interpolate.interp1d(x, _strengths, kind='cubic')(new_x)\n",
    "\n",
    "            # get the right ratio slice of the new list and add it to the grid:\n",
    "            # figure out slice, it should be the same for all\n",
    "            _slice = int(new_length * ratio)\n",
    "            # new_lat = new_lat[:_slice]\n",
    "            # new_long = new_long[:_slice]\n",
    "\n",
    "            active_slice = int(new_length / 33.0)\n",
    "\n",
    "            new_strs = np.zeros(shape=(new_length, 1), dtype=np.int16)\n",
    "\n",
    "            # add the baseline value to the trail to make a sort of 'rail' for\n",
    "            # the light to ride on. This will help enable the looping?\n",
    "            new_strs += 2\n",
    "\n",
    "            # create the highlighted meteor part\n",
    "            active = np.linspace(2, 255, active_slice).astype(np.int16).reshape(active_slice, 1)\n",
    "\n",
    "            # now replace the existing new_strs with the active value using\n",
    "            # _slice as the location to add\n",
    "            if _slice > active_slice:\n",
    "                new_strs[_slice-active_slice:_slice] = active\n",
    "\n",
    "            # if limit_slice < _slice:\n",
    "            #     bright = list(np.linspace(2, 255, active_slice))\n",
    "            #     dim = [2 for idx in range(_slice - limit_slice)]\n",
    "            #     dim += bright\n",
    "            #\n",
    "            #     new_strs = np.array(dim)\n",
    "            #\n",
    "            # else:\n",
    "            #     new_strs = np.linspace(2, 255, _slice)\n",
    "\n",
    "            for idx in range(new_length-1):\n",
    "                if 10 <= new_lat[idx] <= 50 and -110 <= new_long[idx] <= -30:\n",
    "                    # figure out the X/Y coordinates for the current lat/long location\n",
    "                    _Y = int(50 * grid_scale) - 2 - int(new_lat[idx] * grid_scale)\n",
    "                    # was a -1 adjustment because counting starts at 0, changing to -2 because\n",
    "                    # storms tend to start on the right side and not hit the left side\n",
    "                    # so this is a hack-ey fix for what seems like a rounding problem\n",
    "                    _X = int(110 * grid_scale) - 2 + int(new_long[idx] * grid_scale)\n",
    "\n",
    "                    # if temp_grid[_Y, _X] < new_strs[idx]:\n",
    "                    #     # print \"new strs idx:\", new_strs.shape\n",
    "                    temp_grid[_Y, _X] = new_strs[idx]\n",
    "\n",
    "    # keep the return at base level so even if it's a very short storm track\n",
    "    # return something, otherwise you return None and things get unhappy\n",
    "    return temp_grid\n",
    "\n",
    "def create_map_buffer(grid, gam1=1.0, gam2=2.0, file_name=None, color_map=0, year=0):\n",
    "    '''\n",
    "    Render a frame of a heatmap from the grid provided then return it as a buffer\n",
    "    in order to be processed and added to a video\n",
    "    '''\n",
    "\n",
    "    color_dict = {0:\"inferno\", 1:\"plasma\", 2:\"magma\", 3:\"viridis\", 4:\"hot\", 5:\"afmhot\",\n",
    "                 6:\"gist_heat\", 7:\"copper\", 8:\"bone\", 9:\"gnuplot\", 10:\"gnuplot2\",\n",
    "                 11:\"CMRmap\", 12:\"pink\", 13:\"spring\", 14:\"autumn\", 15:\"cool_r\",\n",
    "                 16:\"Wistia_r\", 17:\"seismic\", 18:\"RdGy_r\", 19:\"BrBG_r\", 20:\"RdYlGn_r\",\n",
    "                 21:\"PuOr\", 22:\"brg\", 23:\"hsv\", 24:\"cubehelix\", 25:\"gist_earth\",\n",
    "                 26:\"ocean\", 27:\"gist_stern\", 28:\"gist_rainbow_r\", 29:\"jet\",\n",
    "                 30:\"nipy_spectral\", 31:\"gist_ncar\"}\n",
    "\n",
    "    _cmap = color_dict[color_map]\n",
    "\n",
    "    # establish the figure\n",
    "    fig = plt.figure(figsize=(19.2, 10.8), dpi=100)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    ax.set_facecolor(\"#000000\")\n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10, 50.0)\n",
    "\n",
    "    # make a high peg for the grid to normalize through the years\n",
    "    # for grid size 0.5 97500 was the max\n",
    "    # grid[-1, -1] = 6000\n",
    "\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    _heatmap = np.zeros(shape=(w, h, 4))\n",
    "\n",
    "    # ax.imshow(grid, cmap=_cmap, extent=[-110, -30, 10, 50], alpha=1.0, aspect=\"auto\")\n",
    "    ax.imshow(grid, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-110, -30, 10, 50], alpha=1.0, aspect=\"auto\")\n",
    "\n",
    "    # paint the canvas\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # pull the paint back off the canvas into the buffer\n",
    "    heat_img = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "\n",
    "    # print \"max heat buffer:\", heat_img[...,2].max()\n",
    "\n",
    "    # and black parts transparent (if they're not already)\n",
    "    heat_img[((heat_img[...,0] <= 5) & (heat_img[...,1] <= 5) & (heat_img[...,2] <= 5))] = 0\n",
    "\n",
    "    # fix the super hot square from normalizing throughout the years\n",
    "    # heat_img[((heat_img[...,0] == 255) & (heat_img[...,1] == 255) & (heat_img[...,2] == 255))] = 0\n",
    "\n",
    "    # heat_img[...,0] = heat_img[..., 0] / 10.0\n",
    "    # heat_img[...,1] = heat_img[..., 1] / 10.0\n",
    "    # heat_img[...,2] = heat_img[..., 2] / 10.0\n",
    "\n",
    "    map_image = imread(\"../data/ultra_map.png\")\n",
    "\n",
    "    ax.imshow(map_image, extent=[-110, -30, 10, 50], aspect=\"auto\", alpha=1.0)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    map_buffer = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "\n",
    "    final_buffer = map_buffer + heat_img\n",
    "\n",
    "    final_buffer = np.clip(final_buffer, 0, 255) # clip buffer back into int8 range\n",
    "                    # wonder if some kind of exp transform\n",
    "                    # might enable hdr-like effect\n",
    "\n",
    "    ax.imshow(final_buffer.astype(np.uint8), extent=[-110, -30, 10, 50], aspect=\"auto\", alpha=1.0)\n",
    "    # write out file info so we can see how a map was made later w/ grid search\n",
    "    # desc = \"grid_scale: {:2.2f}\\n gam1: {:2.2f}\\n gam2: {:2.2f}\\n _cmap: {}\".format(7.0, gam1, gam2, _cmap)\n",
    "    if year != 0:\n",
    "        desc = str(year)\n",
    "    else:\n",
    "        desc = \"\"\n",
    "    # ax.annotate(desc, xy=(-109, 48), size=40, color='#AAAAAA')\n",
    "    ax.annotate(\"@pixelated_brian\", xy=(-109, 12), size=20, color=\"#BBBBBB\")\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    final_buffer = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "\n",
    "    print \"max create_map_buffer:\", final_buffer[...,1].max()\n",
    "    print \"create_map_buffer.shape\", final_buffer.shape\n",
    "\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    return final_buffer, desc\n",
    "\n",
    "\n",
    "def draw_map(grid, gam1=1.0, gam2=2.0, file_name=None, color_map=0, year=0):\n",
    "    '''\n",
    "    Save a png file as a map that represents the heatmap of the grid provided\n",
    "    '''\n",
    "\n",
    "    color_dict = {0:\"inferno\", 1:\"plasma\", 2:\"magma\", 3:\"viridis\", 4:\"hot\", 5:\"afmhot\",\n",
    "                 6:\"gist_heat\", 7:\"copper\", 8:\"bone\", 9:\"gnuplot\", 10:\"gnuplot2\",\n",
    "                 11:\"CMRmap\", 12:\"pink\", 13:\"spring\", 14:\"autumn\", 15:\"cool_r\",\n",
    "                 16:\"Wistia_r\", 17:\"seismic\", 18:\"RdGy_r\", 19:\"BrBG_r\", 20:\"RdYlGn_r\",\n",
    "                 21:\"PuOr\", 22:\"brg\", 23:\"hsv\", 24:\"cubehelix\", 25:\"gist_earth\",\n",
    "                 26:\"ocean\", 27:\"gist_stern\", 28:\"gist_rainbow_r\", 29:\"jet\",\n",
    "                 30:\"nipy_spectral\", 31:\"gist_ncar\"}\n",
    "\n",
    "    _cmap = color_dict[color_map]\n",
    "\n",
    "    # establish the figure\n",
    "    fig = plt.figure(figsize=(19.2, 10.8), dpi=100)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.clear()  # maybe clear erases some of the axis settings, not just the canvas?\n",
    "    fig.subplots_adjust(0, 0, 1, 1)\n",
    "    ax.set_facecolor(\"#000000\")\n",
    "    ax.set_xlim(-110.0, -30.0)\n",
    "    ax.set_ylim(10, 50.0)\n",
    "\n",
    "    # make a high peg for the grid to normalize through the years\n",
    "    # for grid size 0.5 97500 was the max\n",
    "    # grid[-1, -1] = 100000\n",
    "\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    _heatmap = np.zeros(shape=(w, h, 4))\n",
    "\n",
    "    ax.imshow(grid, norm=PowerNorm(gamma=gam1/gam2), cmap=_cmap, extent=[-110, -30, 10, 50], alpha=1.0, aspect=\"auto\")\n",
    "\n",
    "    # paint the canvas\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # pull the paint back off the canvas into the buffer\n",
    "    heat_img = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "\n",
    "    # print \"max heat buffer:\", heat_img[...,2].max()\n",
    "\n",
    "    # and black parts transparent (if they're not already)\n",
    "    heat_img[((heat_img[...,0] <= 5) & (heat_img[...,1] <= 5) & (heat_img[...,2] <= 5))] = 0\n",
    "\n",
    "    # fix the super hot square from normalizing throughout the years\n",
    "    # heat_img[((heat_img[...,0] == 255) & (heat_img[...,1] == 255) & (heat_img[...,2] == 255))] = 0\n",
    "\n",
    "    # heat_img[...,0] = heat_img[..., 0] / 10.0\n",
    "    # heat_img[...,1] = heat_img[..., 1] / 10.0\n",
    "    # heat_img[...,2] = heat_img[..., 2] / 10.0\n",
    "\n",
    "    map_image = imread(\"../data/ultra_map.png\")\n",
    "\n",
    "    ax.imshow(map_image, extent=[-110, -30, 10, 50], aspect=\"auto\", alpha=1.0)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    map_buffer = np.frombuffer(fig.canvas.buffer_rgba(), np.uint8).astype(np.int16).reshape(h, w, -1)\n",
    "\n",
    "    final_buffer = map_buffer + heat_img\n",
    "\n",
    "    final_buffer = np.clip(final_buffer, 0, 255) # clip buffer back into int8 range\n",
    "                    # wonder if some kind of exp transform\n",
    "                    # might enable hdr-like effect\n",
    "\n",
    "    ax.imshow(final_buffer.astype(np.uint8), extent=[-110, -30, 10, 50], aspect=\"auto\", alpha=1.0)\n",
    "    # write out file info so we can see how a map was made later w/ grid search\n",
    "    # desc = \"grid_scale: {:2.2f}\\n gam1: {:2.2f}\\n gam2: {:2.2f}\\n _cmap: {}\".format(7.0, gam1, gam2, _cmap)\n",
    "    if year != 0:\n",
    "        desc = str(year)\n",
    "    else:\n",
    "        desc = \"\"\n",
    "    ax.annotate(desc, xy=(-109, 48), size=40, color='#AAAAAA')\n",
    "    ax.annotate(\"@pixelated_brian\", xy=(-109, 12), size=20, color=\"#BBBBBB\")\n",
    "\n",
    "    if file_name != None:\n",
    "        fig.savefig(\"../imgs/test/firebur/{}\".format(file_name), pad_inches=0, transparent=True)\n",
    "\n",
    "\n",
    "def run_map_grid_search(grid, _start=0, _end=1000):\n",
    "    '''\n",
    "    Pick hyper parameters to use in a randomized grid search to try to\n",
    "    find a good looking heatmap setup\n",
    "    '''\n",
    "\n",
    "    for rdx in range(_start, _end):\n",
    "        print \"Making map [{:0>4}]/1000\\r\".format(rdx),\n",
    "        _filename = \"cmap_gsearch_num{:0>4d}.png\".format(rdx)\n",
    "\n",
    "        # # pick grid_scale from lognormal distribution\n",
    "        # # with these values min/max of 10,000 test sample is ~0.5, 21.4\n",
    "        # mu, sigma = 1.2, 0.5\n",
    "        # # returns a numpy array so steal the first value in that to actually get the value\n",
    "        # _grid_scale = np.random.lognormal(mu, sigma, 1)[0]\n",
    "\n",
    "        # gamma 1: pull from a normal distribution centered around 1.0\n",
    "        norm_mu, norm_sigma = 1.5, 0.5\n",
    "\n",
    "        _gam1 = np.random.normal(norm_mu, norm_sigma, 1)[0]\n",
    "\n",
    "        # we don't want the value to go negative so if it is negative set it to 1.0\n",
    "        if _gam1 <= 0:\n",
    "            _gam1 = 1.0\n",
    "\n",
    "        #, gamma 2, cmap? if so what cmap?\n",
    "        g2_mu, g2_sigma = 3.0, 1.0\n",
    "\n",
    "        do_over = 0\n",
    "\n",
    "        _gam2 = -10\n",
    "\n",
    "        # we want to keep picking until _gam2 is bigger than _gam1\n",
    "        # but prevent infinite loops because that's really annoying\n",
    "        while _gam2 < _gam1 or do_over < 1000:\n",
    "            # break infinite loop\n",
    "            do_over += 1\n",
    "            _gam2 = np.random.normal(g2_mu, g2_sigma, 1)[0]\n",
    "\n",
    "        # about 2/3rds the time use the non-standard color map\n",
    "        _color_map = np.random.randint(0, 32)\n",
    "\n",
    "    #     draw_map(grid_scale, x, y, _xs, _ys, gam1=1.0, gam2=2.0, file_name=None, color_map=0)\n",
    "        draw_map(grid, _gam1, _gam2, color_map=_color_map, file_name=_filename)\n",
    "\n",
    "\n",
    "def make_historical_diagram():\n",
    "    '''\n",
    "    do all of the stuff for the graphics of making a hurricane plot\n",
    "\n",
    "    takes in the year dataframe in order to get the necessary data\n",
    "\n",
    "    returns ax which is the figure axis that the current hurricane track will be added upon\n",
    "    '''\n",
    "    # establish the figure\n",
    "    figure = plt.figure(figsize=(19.2, 10.80), dpi=100)\n",
    "\n",
    "    axis = figure.add_subplot(111)\n",
    "    axis.set_facecolor(\"#000000\")\n",
    "\n",
    "#     figure, axis = plt.subplots(figsize=(19.2,10.800), dpi=100)\n",
    "\n",
    "#     data = np.linspace(165.0, 0, 10000).reshape(100,100)\n",
    "# #     data = np.clip(randn(250, 250), -1, 1)\n",
    "\n",
    "#     histo_image = axis.imshow(data, interpolation='nearest', cmap=\"inferno\")\n",
    "\n",
    "#     divider = make_axes_locatable(axis)\n",
    "\n",
    "#     cax = divider.append_axes(\"right\", size=\"2%\", pad=0.05)\n",
    "\n",
    "#     # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "#     cbar = figure.colorbar(histo_image, ticks=[157, 130, 111, 96, 74, 39, 0], cax=cax)\n",
    "#     cbar.ax.set_yticklabels(['5^', '4^', '3^', '2^', '1^', 'T.S.^', 'T.D.^'])  # vertically oriented colorbar\n",
    "#     axis.set_title(\"North American Hurricane Tracks \" + str(_year), size=20)\n",
    "#     axis.set_xlabel(\"Longitude\", size=16)\n",
    "#     axis.set_ylabel(\"Latitude\", size=16)\n",
    "#     axis.set_facecolor(\"black\")\n",
    "    axis.set_xlim(-110.0, -30.0)\n",
    "    axis.set_ylim(0, 50.0)\n",
    "\n",
    "    return figure, axis\n",
    "\n",
    "def heatmap(ax, storm):\n",
    "    '''\n",
    "    make a heatmap of storm track?\n",
    "    '''\n",
    "    x, y = storm_heat(storm)\n",
    "\n",
    "    ax.hexbin(x, y, gridsize=50, bins=\"log\", cmap=\"inferno\")\n",
    "\n",
    "    # ax.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865\n",
      " 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880\n",
      " 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895\n",
      " 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910\n",
      " 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925\n",
      " 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940\n",
      " 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955\n",
      " 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985\n",
      " 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000\n",
      " 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015]\n"
     ]
    }
   ],
   "source": [
    "book_of_storms = get_map_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storm_data = make_storm_data(book_of_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max Length: 2832\n",
      "Mean Length: 657.114155251\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "\n",
    "for idx in range(len(storm_data)):\n",
    "    sizes.append(len(storm_data[idx]))\n",
    "\n",
    "print \"Max Length:\", np.array(sizes).max()\n",
    "print \"Mean Length:\", np.array(sizes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = make_storm_data(book_of_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length: 2832\n",
      "Mean Length: 657.114155251\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "\n",
    "for idx in range(len(new_data)):\n",
    "    sizes.append(len(new_data[idx]))\n",
    "\n",
    "print \"Max Length:\", np.array(sizes).max()\n",
    "print \"Mean Length:\", np.array(sizes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
